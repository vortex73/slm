# -*- coding: utf-8 -*-
"""SLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ydszK5yFUoi6GHiXhuI5dnExs4hWT8l
"""

!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

with open('input.txt','r',encoding='utf-8') as file:
    text = file.read()

print(text[:1000])

l = sorted(list(set(text)))
size = len(l)
print("".join(l))
print(size)

stoi = {ch:i for i,ch in enumerate(l) }
print(stoi)

stoi = {ch:i for i,ch in enumerate(l)}
itos = {i:ch for i,ch in enumerate(l)}
encode = lambda s : [stoi[c] for c in s]
decode = lambda n: ''.join([itos[c] for c in n])

encoded = encode("This is so cool")
print(encoded)
decode(encoded)



import torch
data = torch.tensor(encode(text),dtype=torch.long)
print(data.shape,data.dtype)
print(data[:1000])

num = int(0.9*len(data))
train = data[:num]
test = data[num:]

blksiz = 8
train[:blksiz+1]

len(data)

torch.manual_seed(1337)
batch_size = 4
block_size = 8

def get_batch(split):
    data = train if split=='train' else test
  ix = torch.randint(len(data)-block_size,(batch_size,))
  x = torch.stack([data[i:i+block_size] for i in ix])
  y = torch.stack([data[i+1:i+block_size+1] for i in ix])
  return x,y

xb,yb = get_batch('train')
print(xb)
print(yb)

for a in range(batch_size):
    for b in range(block_size):
        context = xb[a,:b+1]
    result = yb[a,b]
    print(f"Context : {context.tolist()} -> result : {result}")

import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

class bigram(nn.Module):
    def __init__(self,vocab_size):
        super().__init__()
    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)

  def forward(self,idx,targets):
      logits = self.token_embedding_table(idx)
    B, T, C = logits.shape
    logits = logits.view(B*T, C)
    targets = targets.view(B*T)
    loss = F.cross_entropy(logits,targets)
    return logits,loss

m = bigram(size)
logits, loss = m(xb,yb)
print(logits.shape)
print(loss)

